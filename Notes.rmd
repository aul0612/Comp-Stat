---
title: "Notes"
output: 
  html_document:
    code_folding: show
    theme:
      bg: "#202123"
      fg: "#B8BCC2"
      primary: "#EA80FC"
      secondary: "#00DAC6"
      base_font:
        google: Prompt
      heading_font:
        google: Proza Libre
---

```{r setup, include=FALSE}
if (requireNamespace("thematic")) 
  thematic::thematic_rmd(font = "auto")
```
# 25.04.2024
## 2.3 The true view on the EM-Algorithm
### 2.3.1
${P(Z_{ig} = 1) = \pi{_g}}$, but ${\pi_g}$ ist still unknown, as it is one parameter, we want to estimate.

### 2.3.2
Expectation comes in from the posterior probabilities which is equal to a conditional mean since $Z_ig$ is binary.

### 2.3.3
If we maximize log-likelihood for each group individually, no singularities will appear.

We don't know, $Z_{ig}$, so we predict it as a conditional expected value.
This, then is just $p_{ig}$, since the posterior probability is just a conditional expected value.

We first calculate an expected log-likelihood function, where posterior is predictor of latent variable.
Then, we maximize over the parameters, and update our guesses (including the prior distribution) of the parameters.
Then, loop.


# 06.05.2024
## 3.2 Recap
ECDF ($F_n$) is a random function.
right-continuous
ECDF is distribution function of $X^*$ where $X^*$ is discrete with the values we observed.
Random sample comes from random variable X, our ECDF uses $X^*$ which comes from our random sample.
Idea: X ~ F. we cannot sample from F, but we can resample from a distribution with ECDF.
Glivenko-Cantelli-Theorem implies pointwise convergence.

Bootstrap sampling is sampling from the original sample with replacement.
Resampling m times a Bootstrap sample of size n.
with large m theta hat asterisk n is approximating sample well. If initial sample size n is large enough, it is by asymptotic arguments also informative of the population.
Bootstrap is nonparametric because F is not parameterized (there is a parametrical Bootstrap, which is not used often since it requires distributional assumptions on F).
$S_n$ can be either a random sample or the realization of it.


# 13.05.2024
The ECDF is binomial regardless of the actual distribution of X. -> "distribution-free"
That is because rank-statistics are distribution-free

## Exercises Chapter 2

you could use that to conduct cluster Analysis on mnist

## 3.4 The Basic Bootstrap Method

We do as if we know $H_n^{Boot}$, although it is a population version. Since we can choose m arbitrarily large sample version $H_{n,m}^{Boot}$ is arbitrarily good approximation.

# 27.05.2024
we can take the sum out of the variance since we have iid data and thus, the covariances are zero.
big O -> LHS has same order of magnitude
small o -> LHS has smaller order of magnitude


# 03.06.2024
now, lambda_n is bound from above by a conditional mean, i.e. it is stochastic.

# 10.06.2024
If we have a proper 1-$/alpha$ CI, the non-coverage probability is alpha. We want that the non-coverage probability minus alpha goes to zero.
First and second order accuracy describe how fast this convergence is.

With Bootstrap confidence intervals for beta, there might occur multiple testing problems. (-> Bonferoni)

Here, we assume we are only interested in the j-th component of beta.

CI based on Bootstrap pairs is automatically Heteroscedasticity-robust. 

# 13.06.2024
First method of Wild Bootstrap uses that observed residuals are informative about the individual subjects error variance.

Bootstrap t CI: SE used here only works for homoskedastic error term, since it is based on the asymptotic distribution of beta given homoskedastic errors.

## Chapter 4: Nonparametric Regression

# 17.06.2024
Nonparametric Regression assumes, whenever we assume a particular model structure, we are making systematic errors, since it will never be the true model.
-> model misspecification error

with more and more datapoints (large n), we have approxiamtely equidistant design.


# 20.06.2024
Difference to parametric regression setting is that we acknowledge systematic errors.
In parametric regression we would say m(x) is = (not approx.) to our regression model, since we assumes our parametrization is true.
This implies that we even on average make errors. We can see systematic errors in plots of polynomial regressions.

Beta hat is here not really interpretable.

Concrete construction of splines not in the exam.

# 24.06.2024
With splines, we fix the degree of the polynomial and the number of knots serves as something like a smoothing parameter.

The space of spline function includes all linear combinations of the basis functions.

B-splines: We have for each knot-interval k basis functions that we can use and combine linearly to fit out regression.


# 27.06.204
with regression splines the beta hat vector is the linear parameter vector that applied to our basis functions yields the leas squared difference from the true model.
that is, the spline function in the spline space that gets closest to the true outcomes.

m hat depends on the number of knots we use. Different number of knots -> different X matrices -> different m hat

!definitely a big O / small o question in the exam!

# 01.07.2024
With degree k, in the first interval we need to estimate k+1 parameters. In every following interval, only one new parameter is added (parameters of all other basis functions were already fixed in intervals before)

# 04.07.2024
In fixed designs: Bias squared is bound from above by a constant times p^(-4) -> p is smaller than n (to be able to solve OLS problem), but with large n, Bias goes to zero.
Random design, this means squared bias is bound from above in probability by a constant * p^(-4) -> requires that random variable only rarely exceeds the constant.

trace = sum over main diagonal

MSE in parametric regression converges with rate 1/n as Variance converges with rate 1/n and Bias is 0.

Price to pay in nonparametric regression: slower convergence. -> data-hungriness of nonparametric regression: we need more data for the same accuracy.


# 08.07.2024
Leaving out i-th observation in estimating m, prevents prediction error to behave like R^2 (always larger if more parameters included)

GCV is not really cross-validation, rather an information criterion with a penalty term $1/(1-p/n)^2$
Getting the penalty term is not always obvious for different methods.

Overfitting -> outside validity problems

# 15.07.2024
In nonparametric regression we could make ARSS(p) arbitrarily small by increasing p. Thats why this is not a good estimator of sigma^2.

ARSS measures prediction error, MASE measures estimation error, hence, the optimism term.

Magnitude of optimism depends on the noise sigma^2. If that is low, Overfitting isn't as bad.


The optimism part also exists in classic regression. It gives a relationship between the parameters to be estimated (in classic regression analysis p=k) and the sample size n.
p/n has to be such that the optimism term is roughly zero.
